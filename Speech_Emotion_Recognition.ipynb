{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **# Speech Emotion Recognition**"
      ],
      "metadata": {
        "id": "KtIBXV-qRHPu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF1cCRq3u6up"
      },
      "outputs": [],
      "source": [
        "import os #for interacting with the file system\n",
        "import librosa #for music and audio analysis . Librosa helps to visualise audio signals and also\n",
        "#extract feautures using different signal processing techniques\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential #Each layer gives an input to the next layer & the next layer finds some imp feautures and pass it to the next\n",
        "from tensorflow.keras.layers import LSTM,Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJVqh8rrIiF9",
        "outputId": "73b8e155-c2d9-472b-ea53-72d591f90792"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPklHQxKIbVk"
      },
      "outputs": [],
      "source": [
        "dataset_folder = '/content/drive/MyDrive/ravdess/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VR2IIZE8Jj0r"
      },
      "outputs": [],
      "source": [
        "# Define the list of actors in the dataset\n",
        "actors = ['Actor_{}'.format(str(i).zfill(2)) for i in range(1, 25)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "petC6KBfJmCp"
      },
      "outputs": [],
      "source": [
        "# Define the list of emotions\n",
        "emotions = {\n",
        "    '01': 0,  # neutral\n",
        "    '02': 1,  # calm\n",
        "    '03': 2,  # happy\n",
        "    '04': 3,  # sad\n",
        "    '05': 4,  # angry\n",
        "    '06': 5,  # fearful\n",
        "    '07': 6,  # disgust\n",
        "    '08': 7   # surprised\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMj5bdUYJtK8"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Iterate over the actors and load the audio files\n",
        "for actor in actors:\n",
        "    actor_folder = os.path.join(dataset_folder, actor)\n",
        "    for file in os.listdir(actor_folder):\n",
        "        if file.endswith('.wav'):\n",
        "            file_path = os.path.join(actor_folder, file)\n",
        "            # Load the audio file\n",
        "            audio_data, _ = librosa.load(file_path, sr=None)\n",
        "            # Extract features (e.g., MFCCs)\n",
        "            mfcc = librosa.feature.mfcc(y=audio_data, sr=16000, n_mfcc=13)\n",
        "            # Flatten the feature matrix to create a feature vector\n",
        "            flattened_mfcc = np.ravel(mfcc)\n",
        "            features.append(flattened_mfcc)\n",
        "            emotion_label = file.split('-')[2]\n",
        "            labels.append(emotions[emotion_label])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgjVGHkJQKcU"
      },
      "outputs": [],
      "source": [
        "# Pad the feature sequences to ensure consistent length\n",
        "features_padded = pad_sequences(features, dtype='float32', padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hApVhW9KVRk"
      },
      "outputs": [],
      "source": [
        "# Convert the lists to NumPy arrays\n",
        "features = np.array(features_padded)\n",
        "labels = np.array(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdiEoynvKZcO"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGaR5ovoWXSD"
      },
      "outputs": [],
      "source": [
        "# Reshape the target labels to match the shape of the predicted output\n",
        "y_train = y_train.reshape((y_train.shape[0], 1))\n",
        "y_test = y_test.reshape((y_test.shape[0], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU5h5pILRBTd"
      },
      "outputs": [],
      "source": [
        "# Encode the emotion labels\n",
        "\n",
        "\n",
        "num_classes = len(np.unique(labels))\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKil7nk1Rja1"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=128, input_shape=input_shape))\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XIKXNZERvWN"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V32Rq6fMR2cX",
        "outputId": "1ced54b5-e97f-4248-983e-237dd076d519"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "36/36 [==============================] - 99s 3s/step - loss: 2.0725 - accuracy: 0.1276 - val_loss: 2.0742 - val_accuracy: 0.0833\n",
            "Epoch 2/10\n",
            "36/36 [==============================] - 95s 3s/step - loss: 2.0652 - accuracy: 0.1215 - val_loss: 2.0800 - val_accuracy: 0.0833\n",
            "Epoch 3/10\n",
            "36/36 [==============================] - 95s 3s/step - loss: 2.0614 - accuracy: 0.1467 - val_loss: 2.0744 - val_accuracy: 0.0833\n",
            "Epoch 4/10\n",
            "36/36 [==============================] - 93s 3s/step - loss: 2.0618 - accuracy: 0.1467 - val_loss: 2.0758 - val_accuracy: 0.0833\n",
            "Epoch 5/10\n",
            "36/36 [==============================] - 95s 3s/step - loss: 2.0591 - accuracy: 0.1467 - val_loss: 2.0774 - val_accuracy: 0.0833\n",
            "Epoch 6/10\n",
            "36/36 [==============================] - 93s 3s/step - loss: 2.0602 - accuracy: 0.1380 - val_loss: 2.0756 - val_accuracy: 0.0833\n",
            "Epoch 7/10\n",
            "36/36 [==============================] - 95s 3s/step - loss: 2.0598 - accuracy: 0.1467 - val_loss: 2.0744 - val_accuracy: 0.0833\n",
            "Epoch 8/10\n",
            "36/36 [==============================] - 94s 3s/step - loss: 2.0601 - accuracy: 0.1467 - val_loss: 2.0761 - val_accuracy: 0.0833\n",
            "Epoch 9/10\n",
            "36/36 [==============================] - 93s 3s/step - loss: 2.0591 - accuracy: 0.1467 - val_loss: 2.0761 - val_accuracy: 0.0833\n",
            "Epoch 10/10\n",
            "36/36 [==============================] - 94s 3s/step - loss: 2.0594 - accuracy: 0.1424 - val_loss: 2.0755 - val_accuracy: 0.0833\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c8786747a00>"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(X_train, y_train, batch_size=32, epochs=10, validation_data=(X_test, y_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}